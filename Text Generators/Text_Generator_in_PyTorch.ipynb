{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "5UOMpNQtBZYf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"Once upon a time, in a faraway kingdom, there was a kind and beautiful princess named Snow White. \"\n",
        "        \"She had skin as white as snow, lips as red as roses, and hair as black as coal. \"\n",
        "        \"But she lived with her stepmother, the Queen, who was beautiful on the outside but jealous and cruel on the inside.\")\n"
      ],
      "metadata": {
        "id": "taKCP0OlFfkf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "char_to_index = {char: i for i, char in enumerate(chars)}\n",
        "index_to_char = {i: char for i, char in enumerate(chars)}"
      ],
      "metadata": {
        "id": "7M6NDzxSF8TT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 3\n",
        "sequences = []\n",
        "labels = []"
      ],
      "metadata": {
        "id": "tDbPLtA_Gkk9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(text)-seq_length):\n",
        "  seq = text[i:i + seq_length]\n",
        "  label = text[i + seq_length]\n",
        "  sequences.append([char_to_index[char] for char in seq])\n",
        "  labels.append(char_to_index[label])"
      ],
      "metadata": {
        "id": "tM-EqeEiGwCK"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(sequences)\n",
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "exT8wtRMHK-p"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tensor = torch.from_numpy(x)\n",
        "y_tensor = torch.from_numpy(y)"
      ],
      "metadata": {
        "id": "Ip-dJ1uJHTmR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = torch.nn.functional.one_hot(x_tensor, num_classes = len(chars)).float()"
      ],
      "metadata": {
        "id": "1BZ2RMDaHdTN"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TensorDataset(x_one_hot, y_tensor)\n",
        "batch = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "tADRyEGFH4O8"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharLSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "     super(CharLSTM, self).__init__()\n",
        "     self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "     self.fc = nn.Linear(hidden_size, num_classes)\n",
        "  def forward(self, x):\n",
        "     out, _ = self.lstm(x)\n",
        "     out = out[:, -1, :]\n",
        "     out = self.fc(out)\n",
        "     return out\n"
      ],
      "metadata": {
        "id": "lp2ZQEBhIP5T"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(chars)\n",
        "hidden_size = 160\n",
        "num_layers = 6\n",
        "num_classes = len(chars)\n",
        "num_epochs = 300\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "QZlp1DI5JT0h"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharLSTM(input_size, hidden_size, num_layers, num_classes)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "u9VWMH2vKlzE"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  for batch_x, batch_y in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(batch_x)\n",
        "    loss = criterion(outputs, batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  avg_loss = total_loss/len(dataloader)\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss: .4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7UBJVx4K7u5",
        "outputId": "60833f58-da46-44b4-dc06-6afa0df4c3be"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss:  3.3970\n",
            "Epoch 2/300, Loss:  3.1285\n",
            "Epoch 3/300, Loss:  2.9752\n",
            "Epoch 4/300, Loss:  2.9870\n",
            "Epoch 5/300, Loss:  2.8940\n",
            "Epoch 6/300, Loss:  3.0288\n",
            "Epoch 7/300, Loss:  2.8870\n",
            "Epoch 8/300, Loss:  2.8605\n",
            "Epoch 9/300, Loss:  2.8354\n",
            "Epoch 10/300, Loss:  2.9352\n",
            "Epoch 11/300, Loss:  2.8447\n",
            "Epoch 12/300, Loss:  2.8204\n",
            "Epoch 13/300, Loss:  2.9989\n",
            "Epoch 14/300, Loss:  2.9361\n",
            "Epoch 15/300, Loss:  2.8231\n",
            "Epoch 16/300, Loss:  2.9554\n",
            "Epoch 17/300, Loss:  2.9067\n",
            "Epoch 18/300, Loss:  2.9143\n",
            "Epoch 19/300, Loss:  2.9502\n",
            "Epoch 20/300, Loss:  3.0056\n",
            "Epoch 21/300, Loss:  2.9799\n",
            "Epoch 22/300, Loss:  2.9381\n",
            "Epoch 23/300, Loss:  2.9061\n",
            "Epoch 24/300, Loss:  2.8641\n",
            "Epoch 25/300, Loss:  2.8541\n",
            "Epoch 26/300, Loss:  2.9122\n",
            "Epoch 27/300, Loss:  2.8390\n",
            "Epoch 28/300, Loss:  2.9352\n",
            "Epoch 29/300, Loss:  2.8783\n",
            "Epoch 30/300, Loss:  2.7518\n",
            "Epoch 31/300, Loss:  2.9853\n",
            "Epoch 32/300, Loss:  2.7974\n",
            "Epoch 33/300, Loss:  2.7225\n",
            "Epoch 34/300, Loss:  2.8557\n",
            "Epoch 35/300, Loss:  2.7779\n",
            "Epoch 36/300, Loss:  2.8516\n",
            "Epoch 37/300, Loss:  2.8187\n",
            "Epoch 38/300, Loss:  2.7090\n",
            "Epoch 39/300, Loss:  2.7090\n",
            "Epoch 40/300, Loss:  2.8838\n",
            "Epoch 41/300, Loss:  2.7705\n",
            "Epoch 42/300, Loss:  2.8121\n",
            "Epoch 43/300, Loss:  2.7420\n",
            "Epoch 44/300, Loss:  2.7128\n",
            "Epoch 45/300, Loss:  2.6761\n",
            "Epoch 46/300, Loss:  2.6859\n",
            "Epoch 47/300, Loss:  2.7028\n",
            "Epoch 48/300, Loss:  2.7658\n",
            "Epoch 49/300, Loss:  2.7209\n",
            "Epoch 50/300, Loss:  2.7069\n",
            "Epoch 51/300, Loss:  2.6733\n",
            "Epoch 52/300, Loss:  2.6463\n",
            "Epoch 53/300, Loss:  2.7297\n",
            "Epoch 54/300, Loss:  2.7395\n",
            "Epoch 55/300, Loss:  2.6854\n",
            "Epoch 56/300, Loss:  2.6387\n",
            "Epoch 57/300, Loss:  2.7047\n",
            "Epoch 58/300, Loss:  2.6295\n",
            "Epoch 59/300, Loss:  2.5303\n",
            "Epoch 60/300, Loss:  2.4876\n",
            "Epoch 61/300, Loss:  2.5424\n",
            "Epoch 62/300, Loss:  2.6003\n",
            "Epoch 63/300, Loss:  2.4861\n",
            "Epoch 64/300, Loss:  2.6082\n",
            "Epoch 65/300, Loss:  2.5581\n",
            "Epoch 66/300, Loss:  2.4292\n",
            "Epoch 67/300, Loss:  2.5155\n",
            "Epoch 68/300, Loss:  2.7525\n",
            "Epoch 69/300, Loss:  2.4823\n",
            "Epoch 70/300, Loss:  2.5368\n",
            "Epoch 71/300, Loss:  2.5022\n",
            "Epoch 72/300, Loss:  2.5562\n",
            "Epoch 73/300, Loss:  2.5645\n",
            "Epoch 74/300, Loss:  2.5942\n",
            "Epoch 75/300, Loss:  2.5632\n",
            "Epoch 76/300, Loss:  2.4642\n",
            "Epoch 77/300, Loss:  2.4472\n",
            "Epoch 78/300, Loss:  2.3594\n",
            "Epoch 79/300, Loss:  2.3143\n",
            "Epoch 80/300, Loss:  2.4384\n",
            "Epoch 81/300, Loss:  2.3702\n",
            "Epoch 82/300, Loss:  2.3119\n",
            "Epoch 83/300, Loss:  2.3406\n",
            "Epoch 84/300, Loss:  2.3250\n",
            "Epoch 85/300, Loss:  2.3909\n",
            "Epoch 86/300, Loss:  2.3188\n",
            "Epoch 87/300, Loss:  2.3904\n",
            "Epoch 88/300, Loss:  2.3278\n",
            "Epoch 89/300, Loss:  2.3212\n",
            "Epoch 90/300, Loss:  2.3778\n",
            "Epoch 91/300, Loss:  2.5072\n",
            "Epoch 92/300, Loss:  2.4839\n",
            "Epoch 93/300, Loss:  2.2533\n",
            "Epoch 94/300, Loss:  2.2416\n",
            "Epoch 95/300, Loss:  2.1529\n",
            "Epoch 96/300, Loss:  2.2329\n",
            "Epoch 97/300, Loss:  2.2743\n",
            "Epoch 98/300, Loss:  2.2478\n",
            "Epoch 99/300, Loss:  2.2469\n",
            "Epoch 100/300, Loss:  2.1355\n",
            "Epoch 101/300, Loss:  2.1986\n",
            "Epoch 102/300, Loss:  2.1658\n",
            "Epoch 103/300, Loss:  2.0267\n",
            "Epoch 104/300, Loss:  2.1130\n",
            "Epoch 105/300, Loss:  2.1752\n",
            "Epoch 106/300, Loss:  2.1028\n",
            "Epoch 107/300, Loss:  2.2325\n",
            "Epoch 108/300, Loss:  2.0241\n",
            "Epoch 109/300, Loss:  2.1226\n",
            "Epoch 110/300, Loss:  2.2580\n",
            "Epoch 111/300, Loss:  2.2079\n",
            "Epoch 112/300, Loss:  2.0686\n",
            "Epoch 113/300, Loss:  2.1408\n",
            "Epoch 114/300, Loss:  2.0238\n",
            "Epoch 115/300, Loss:  2.0810\n",
            "Epoch 116/300, Loss:  2.0197\n",
            "Epoch 117/300, Loss:  2.0078\n",
            "Epoch 118/300, Loss:  2.0381\n",
            "Epoch 119/300, Loss:  2.0081\n",
            "Epoch 120/300, Loss:  2.0345\n",
            "Epoch 121/300, Loss:  1.8328\n",
            "Epoch 122/300, Loss:  1.9073\n",
            "Epoch 123/300, Loss:  1.8416\n",
            "Epoch 124/300, Loss:  1.8785\n",
            "Epoch 125/300, Loss:  1.7621\n",
            "Epoch 126/300, Loss:  1.8308\n",
            "Epoch 127/300, Loss:  1.8269\n",
            "Epoch 128/300, Loss:  1.8462\n",
            "Epoch 129/300, Loss:  1.8644\n",
            "Epoch 130/300, Loss:  1.7764\n",
            "Epoch 131/300, Loss:  1.7480\n",
            "Epoch 132/300, Loss:  1.9374\n",
            "Epoch 133/300, Loss:  1.8281\n",
            "Epoch 134/300, Loss:  1.7565\n",
            "Epoch 135/300, Loss:  1.7582\n",
            "Epoch 136/300, Loss:  1.6929\n",
            "Epoch 137/300, Loss:  1.8073\n",
            "Epoch 138/300, Loss:  1.9251\n",
            "Epoch 139/300, Loss:  1.7209\n",
            "Epoch 140/300, Loss:  1.6785\n",
            "Epoch 141/300, Loss:  1.7184\n",
            "Epoch 142/300, Loss:  1.7189\n",
            "Epoch 143/300, Loss:  1.6758\n",
            "Epoch 144/300, Loss:  1.5853\n",
            "Epoch 145/300, Loss:  1.7203\n",
            "Epoch 146/300, Loss:  1.5256\n",
            "Epoch 147/300, Loss:  1.6153\n",
            "Epoch 148/300, Loss:  1.6201\n",
            "Epoch 149/300, Loss:  1.4864\n",
            "Epoch 150/300, Loss:  1.6318\n",
            "Epoch 151/300, Loss:  1.4835\n",
            "Epoch 152/300, Loss:  1.6015\n",
            "Epoch 153/300, Loss:  1.5904\n",
            "Epoch 154/300, Loss:  1.4237\n",
            "Epoch 155/300, Loss:  1.4992\n",
            "Epoch 156/300, Loss:  1.4565\n",
            "Epoch 157/300, Loss:  1.4384\n",
            "Epoch 158/300, Loss:  1.4605\n",
            "Epoch 159/300, Loss:  1.7799\n",
            "Epoch 160/300, Loss:  1.6645\n",
            "Epoch 161/300, Loss:  1.6215\n",
            "Epoch 162/300, Loss:  1.6563\n",
            "Epoch 163/300, Loss:  1.5718\n",
            "Epoch 164/300, Loss:  1.5766\n",
            "Epoch 165/300, Loss:  1.5351\n",
            "Epoch 166/300, Loss:  1.4642\n",
            "Epoch 167/300, Loss:  1.5199\n",
            "Epoch 168/300, Loss:  1.4207\n",
            "Epoch 169/300, Loss:  1.3682\n",
            "Epoch 170/300, Loss:  1.2651\n",
            "Epoch 171/300, Loss:  1.4516\n",
            "Epoch 172/300, Loss:  1.3232\n",
            "Epoch 173/300, Loss:  1.3999\n",
            "Epoch 174/300, Loss:  1.4820\n",
            "Epoch 175/300, Loss:  1.3947\n",
            "Epoch 176/300, Loss:  1.4879\n",
            "Epoch 177/300, Loss:  1.3296\n",
            "Epoch 178/300, Loss:  1.4727\n",
            "Epoch 179/300, Loss:  1.2449\n",
            "Epoch 180/300, Loss:  1.1993\n",
            "Epoch 181/300, Loss:  1.2872\n",
            "Epoch 182/300, Loss:  1.1852\n",
            "Epoch 183/300, Loss:  1.1442\n",
            "Epoch 184/300, Loss:  1.1139\n",
            "Epoch 185/300, Loss:  1.1583\n",
            "Epoch 186/300, Loss:  1.1332\n",
            "Epoch 187/300, Loss:  1.2811\n",
            "Epoch 188/300, Loss:  1.1428\n",
            "Epoch 189/300, Loss:  1.0800\n",
            "Epoch 190/300, Loss:  1.1481\n",
            "Epoch 191/300, Loss:  1.2382\n",
            "Epoch 192/300, Loss:  1.3130\n",
            "Epoch 193/300, Loss:  1.3051\n",
            "Epoch 194/300, Loss:  1.4207\n",
            "Epoch 195/300, Loss:  1.3959\n",
            "Epoch 196/300, Loss:  1.2821\n",
            "Epoch 197/300, Loss:  1.2215\n",
            "Epoch 198/300, Loss:  1.1680\n",
            "Epoch 199/300, Loss:  1.1247\n",
            "Epoch 200/300, Loss:  1.1399\n",
            "Epoch 201/300, Loss:  1.0791\n",
            "Epoch 202/300, Loss:  1.2369\n",
            "Epoch 203/300, Loss:  1.1356\n",
            "Epoch 204/300, Loss:  1.2272\n",
            "Epoch 205/300, Loss:  1.0914\n",
            "Epoch 206/300, Loss:  1.0633\n",
            "Epoch 207/300, Loss:  1.0632\n",
            "Epoch 208/300, Loss:  1.0196\n",
            "Epoch 209/300, Loss:  0.9855\n",
            "Epoch 210/300, Loss:  0.9469\n",
            "Epoch 211/300, Loss:  0.9809\n",
            "Epoch 212/300, Loss:  1.0052\n",
            "Epoch 213/300, Loss:  0.9395\n",
            "Epoch 214/300, Loss:  1.0229\n",
            "Epoch 215/300, Loss:  1.0311\n",
            "Epoch 216/300, Loss:  1.1244\n",
            "Epoch 217/300, Loss:  1.1652\n",
            "Epoch 218/300, Loss:  1.0330\n",
            "Epoch 219/300, Loss:  0.9835\n",
            "Epoch 220/300, Loss:  0.9555\n",
            "Epoch 221/300, Loss:  0.9829\n",
            "Epoch 222/300, Loss:  0.9022\n",
            "Epoch 223/300, Loss:  0.9479\n",
            "Epoch 224/300, Loss:  0.8775\n",
            "Epoch 225/300, Loss:  0.9135\n",
            "Epoch 226/300, Loss:  0.8397\n",
            "Epoch 227/300, Loss:  0.8748\n",
            "Epoch 228/300, Loss:  0.9640\n",
            "Epoch 229/300, Loss:  0.8701\n",
            "Epoch 230/300, Loss:  0.9277\n",
            "Epoch 231/300, Loss:  0.8475\n",
            "Epoch 232/300, Loss:  0.8861\n",
            "Epoch 233/300, Loss:  0.9344\n",
            "Epoch 234/300, Loss:  1.0985\n",
            "Epoch 235/300, Loss:  1.4088\n",
            "Epoch 236/300, Loss:  1.3389\n",
            "Epoch 237/300, Loss:  1.2559\n",
            "Epoch 238/300, Loss:  1.2151\n",
            "Epoch 239/300, Loss:  1.1328\n",
            "Epoch 240/300, Loss:  0.9379\n",
            "Epoch 241/300, Loss:  0.8836\n",
            "Epoch 242/300, Loss:  0.8595\n",
            "Epoch 243/300, Loss:  0.9758\n",
            "Epoch 244/300, Loss:  0.8452\n",
            "Epoch 245/300, Loss:  1.0081\n",
            "Epoch 246/300, Loss:  0.9656\n",
            "Epoch 247/300, Loss:  0.8462\n",
            "Epoch 248/300, Loss:  0.8328\n",
            "Epoch 249/300, Loss:  0.8673\n",
            "Epoch 250/300, Loss:  0.7778\n",
            "Epoch 251/300, Loss:  0.7550\n",
            "Epoch 252/300, Loss:  0.7386\n",
            "Epoch 253/300, Loss:  0.8008\n",
            "Epoch 254/300, Loss:  0.7889\n",
            "Epoch 255/300, Loss:  0.7127\n",
            "Epoch 256/300, Loss:  0.8271\n",
            "Epoch 257/300, Loss:  0.8192\n",
            "Epoch 258/300, Loss:  0.8274\n",
            "Epoch 259/300, Loss:  0.7095\n",
            "Epoch 260/300, Loss:  0.7198\n",
            "Epoch 261/300, Loss:  0.7769\n",
            "Epoch 262/300, Loss:  0.7807\n",
            "Epoch 263/300, Loss:  0.7886\n",
            "Epoch 264/300, Loss:  0.7575\n",
            "Epoch 265/300, Loss:  0.7728\n",
            "Epoch 266/300, Loss:  0.7925\n",
            "Epoch 267/300, Loss:  0.7528\n",
            "Epoch 268/300, Loss:  0.7403\n",
            "Epoch 269/300, Loss:  0.7800\n",
            "Epoch 270/300, Loss:  0.7789\n",
            "Epoch 271/300, Loss:  0.7410\n",
            "Epoch 272/300, Loss:  0.7365\n",
            "Epoch 273/300, Loss:  0.7743\n",
            "Epoch 274/300, Loss:  0.7125\n",
            "Epoch 275/300, Loss:  0.7413\n",
            "Epoch 276/300, Loss:  0.7653\n",
            "Epoch 277/300, Loss:  0.8330\n",
            "Epoch 278/300, Loss:  0.7456\n",
            "Epoch 279/300, Loss:  0.7076\n",
            "Epoch 280/300, Loss:  0.6678\n",
            "Epoch 281/300, Loss:  0.8203\n",
            "Epoch 282/300, Loss:  0.6442\n",
            "Epoch 283/300, Loss:  0.7469\n",
            "Epoch 284/300, Loss:  0.7743\n",
            "Epoch 285/300, Loss:  0.7574\n",
            "Epoch 286/300, Loss:  0.6902\n",
            "Epoch 287/300, Loss:  0.7551\n",
            "Epoch 288/300, Loss:  0.6299\n",
            "Epoch 289/300, Loss:  0.6287\n",
            "Epoch 290/300, Loss:  0.7092\n",
            "Epoch 291/300, Loss:  0.6369\n",
            "Epoch 292/300, Loss:  0.6427\n",
            "Epoch 293/300, Loss:  0.6112\n",
            "Epoch 294/300, Loss:  0.7039\n",
            "Epoch 295/300, Loss:  0.6962\n",
            "Epoch 296/300, Loss:  0.6735\n",
            "Epoch 297/300, Loss:  0.6491\n",
            "Epoch 298/300, Loss:  0.6583\n",
            "Epoch 299/300, Loss:  0.5940\n",
            "Epoch 300/300, Loss:  0.6712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_seq, length, char_to_index, index_to_char):\n",
        "  model.eval()\n",
        "  seq = [char_to_index[c] for c in start_seq]\n",
        "  generated = start_seq\n",
        "  for _ in range(length):\n",
        "    x = torch.tensor([seq[-seq_length:]])\n",
        "    x_onehot = torch.nn.functional.one_hot(x, num_classes=len(chars)).float()\n",
        "    with torch.no_grad():\n",
        "      out = model(x_onehot)\n",
        "      pred = out.argmax(dim=1).item()\n",
        "    generated += index_to_char[pred]\n",
        "    seq.append(pred)\n",
        "  return generated"
      ],
      "metadata": {
        "id": "TOTsqFmeL6A1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text: \")\n",
        "print(generate_text(model, \"Once\", 100, char_to_index, index_to_char))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-CmqiaDNGf6",
        "outputId": "339e6f83-d9d4-47e5-fb34-3425aa029fd5"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: \n",
            "Once upon the had sioeesa, khndaw ahd with  eiu .iu .iu .iu .iu .iu .iu .iu .iu .iu .iu .iu .iu .iu .iu \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcQR3irVNY9v"
      },
      "execution_count": 76,
      "outputs": []
    }
  ]
}